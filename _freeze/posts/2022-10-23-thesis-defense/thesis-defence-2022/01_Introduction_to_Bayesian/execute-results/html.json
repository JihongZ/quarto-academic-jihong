{
  "hash": "87bbbbb366330e84cd16a14d07c7a325",
  "result": {
    "markdown": "---\ntitle: \"Model Selection Posterior Predictive Model Checking\"\nsubtitle: \"Via Limited-information Indices for Bayesian Diagnostic Classification Modeling\"\nauthor: \"Jihong Zhang\" \ndate: 2022-10-12\ntitle-block-banner: true\ntitle-block-banner-color: \"red\"\nformat: \n  revealjs:\n    multiplex: true\n    logo: images/uiowa.png\n    preview-links: auto\n    footer: \"Thesis Defence 2022\"\n    theme: [dark, pp.scss]\n    slide-number: c/t\n    incremental: true\neditor: source\neditor_options: \n  markdown: \n    wrap: 72\n---\n\n\n## Objectives\n\n::: nonincremental\n1.  Background: Why it matters?\n2.  Proposed method\n3.  Simulation study\n4.  Empirical study\n:::\n\n...but, before we formally begin...\n\n## Why Bayesian DCM?\n\n-   Flexible model construction\n\n-   Missing data\n\n-   Natural interpretation\n\n## Why Model Selection in Bayesian DCM?\n\n-   Model selection issue is important in practice:\n\n    -   When true model doesn't exist, which one is better\n\n    -   Simple model vs. Complex model\n\n    -   Test construct is questionable\n\n    -   Competing test theory exist\n\n-   Model selection is affected by:\n\n    -   Sample size\n\n    -   Q-matrix misspecification\n\n    -   Model complexity\n\n## Bayesian from Birth\n\nA brief video to start the semester...\n<a href=\"https://t.co/T9O9UzJVWT\">pic.twitter.com/T9O9UzJVWT</a>\n\n## The Basics of Bayesian Analyses\n\n-   Bayesian statistical analysis refers to the use of models where some\n    or all of the parameters are treated as random components\n    -   Each parameter comes from some type of distribution\n-   The likelihood function of the data is then augmented with an\n    additional term that represents the likelihood of the prior\n    distribution for each parameter\n    -   Think of this as saying each parameter has a certain likelihood\n        -- the height of the prior distribution\n-   The final estimates are then considered summaries of the posterior\n    distribution of the parameter, conditional on the data\n    -   In practice, we use these estimates to make inferences, just as\n        is done when using non-Bayesian approaches (e.g., maximum\n        likelihood/least squares)\n\n## Why are Bayesian Methods Used?\n\n-   Bayesian methods get used because of the *relative* accessibility of\n    one method of estimation (MCMC -- to be discussed shortly)\n\n-   There are four main reasons why people use MCMC:\n\n1.  Missing data\n2.  Lack of software capable of handling large sized analyses\n3.  New models/generalizations of models not available in software\n4.  Philosoplyical Reasons (e.g., membership in the cult of Bayes)\n\n## Perceptions and Issues with Bayesian Methods\n\n-   The use of Bayesian statistics has been controversial, historically\n    (but less so today)\n    -   The use of certain prior distributions can produce results that\n        are biased or reflect subjective judgment rather than objective\n        science\n-   Most MCMC estimation methods are \u000bcomputationally intensive\n    -   Until very recently, very few methods available for those who\n        aren't into programming in Fortran, C, or C++\n-   Understanding of what Bayesian methods had been very limited outside\n    the field of mathematical statistics (but that is changing now)\n-   Over the past 20 years, Bayesian methods have become widespread --\n    making new models estimable and becoming standard in some social\n    science fields (quantitative psychology and educational measurement)\n\n## How Bayesian Statistics Work\n\n<!-- ::: {.nonincremental} -->\n\nBayesian methods rely on Bayes' Theorem <!-- :::  -->\n\n$$P (A \\mid B) = \\frac{P(B\\mid A)P(A)}{P(B)} \\propto P(B\\mid A)P(A)$$\nHere:\n\n::: nonincremental\n-   $P(A \\mid B)$ is the <u>prior distribution</u> (pdf) of A (i.e., WHY\n    THINGS ARE BAYESIAN)\n-   $P(B)$ is the <u>marginal distribution</u> (pdf) of B\n-   $P(B \\mid A)$ is the <u>conditional distribution</u> (pdf) of B,\n    given A\n-   $P (A \\mid B)$is the <u>posterior distribution</u> (pdf) of A, given\n    B\n:::\n\n## A Live Bayesian Example\n\n-   Suppose we wanted to assess the probability of rolling a one on a\n    six-sided die: $$p_1 = P(D=1)$$\n\n-   We then collect a sample of data $\\boldsymbol{X} = \\{0,1,0,1,1 \\}$\n\n    -   These are independent tosses of the die\n\n-   The posterior distribution of the probability of a one conditional\n    on the data is: $$P(p_1 \\mid \\boldsymbol{X})$$\n\n-   We can determine this via Bayes theorem:\n    $$P(p_1 \\mid \\boldsymbol{X}) = \\frac{P(\\boldsymbol{X} \\mid p_1)P(p_1)}{P(\\boldsymbol{X})} \\propto P(\\boldsymbol{X} \\mid p_1)P(p_1)$$\n\n## Defining the Likelihood Function $P(\\boldsymbol{X} \\mid p_1)$\n\nThe likelihood of the data given the parameter:\n\n$$P(\\boldsymbol{X} \\mid p_1) = \\prod_{i=1}^N p_1^{X_i} \\left(1-p_1\\right)^{(1-X_i)}$$\n\n-   Any given roll of the dice $X_i$ is a Bernoulli variable\n    $X_i \\sim B(p_1)$\n    -   A \"success\" is defined by rolling a one\n-   The product in the likelihood function comes from each roll being\n    independent\n    -   The outcome of a roll does not depend on previous or future\n        rolls\n\n## Visualizing the Likelihood Function\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](01_Introduction_to_Bayesian_files/figure-html/likeplot-1.png){width=672}\n:::\n:::\n\n\n## Choosing the Prior Distribution for $p_1$\n\nWe must now pick the prior distribution of $p_1$:\n\n$$P(p_1)$$ ::: {.nonincremental} \\* Our choice is subjective: Many\ndistributions to choose from \\* What we know is that for a \"fair\" die,\nthe probability of rolling a one is $\\frac{1}{6}$ \\* But...probability\nis not a distribution \\* Instead, let's consider a Beta distribution\n$p_1 \\sim Beta\\left(\\alpha, \\beta\\right)$ :::\n\n## The Beta Distribution\n\nFor parameters that range between zero and one (or two finite end\npoints), the Beta distribution makes a good choice for a prior:\n\n$$P(p_1) = \\frac{\\left( p_1\\right)^{\\alpha-1} \\left(1-p_1 \\right)^{\\beta1-1}}{B\\left(\\alpha, \\beta\\right)}, $$\nwhere:\n\n$$B\\left(\\alpha, \\beta\\right) = \\frac{\\Gamma\\left(\\alpha\\right)\\Gamma\\left(\\beta\\right)}{\\Gamma\\left(\\alpha+\\beta\\right)}, $$\nand,\n\n$$\\Gamma\\left(z \\right) = \\int_0^\\infty t^{z-1} e^{-t}dt$$ \\## More Beta\nDistribution\n\nThe Beta distribution has a mean of $\\frac{\\alpha}{\\alpha+\\beta}$\n\n-   The parameters $\\alpha$ and $\\beta$ are called\n    <u>hyperparameters</u>\n    -   Hyperparameters are parameters of prior distributions\n-   We can pick values of $\\alpha$ and $\\beta$ to correspond to\n    $\\frac{1}{6}$\n    -   Many choices: $\\alpha=1$ and $\\beta=5$ have the same mean as\n        $\\alpha=100$ and $\\beta=500$\n-   What is the difference?\n    -   How strongly we feel in our beliefs...as quantified by...\n\n## More More Beta Distribution\n\nThe Beta distribution has a variance of\n$\\frac{\\alpha\\beta}{\\left(\\alpha+\\beta \\right)^2 \\left(\\alpha+\\beta+1 \\right))}$\n\n-   Choosing $\\alpha=1$ and $\\beta=5$ yields a prior with mean\n    $\\frac{1}{6}$ and variance $0.02$\n-   Choosing $\\alpha=100$ and $\\beta=500$ yields a prior with mean\n    $\\frac{1}{6}$ and variance $0.0002$\n-   The smaller prior variance means the prior is more\n    <u>informative</u>\n    -   Informative priors are those that have relatively small\n        variances\n    -   <u>Uninformative</u> priors are those that have relatively large\n        variances\n\n## Visualizing $P(p_1)$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](01_Introduction_to_Bayesian_files/figure-html/priorPlot-1.png){width=672}\n:::\n:::\n\n\n## The Posterior Distribution\n\nChoosing a Beta distribution for a prior for $p_1$ is *very* convenient\n\n-   When combined with Bernoulli (Binomial) data likelihood the\n    posterior distribution can be derived analytically\n-   The posterior distribution is also a Beta distribution\n    -   $\\alpha = a + \\sum_{i=1}^NX_i$ ($a$ is the hyperparameter of the\n        prior distribution)\n    -   $\\beta = b + N - \\sum_{i=1}^NX_i$ ($b$ is the hyperparameter of\n        the posterior distribution)\n-   The Beta prior is said to be a <u>conjugate prior</u>: A prior\n    distribution that leads to a posterior distribution of the same\n    family\n    -   Here, prior == Beta and posterior == Beta\n\n## Visualizing The Posterior Distribution\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](01_Introduction_to_Bayesian_files/figure-html/postPlot-1.png){width=672}\n:::\n:::\n\n\n## Bayesian Estimates are Summaries of the Posterior Distribution\n\nTo determine the estimate of $p_1$, we use summaries of the posterior\ndistribution:\n\n-   With prior hyperparameters $a=1$ and $b=5$\n    -   $\\hat{p}_1 = \\frac{1+3}{1+3 +5+2} = \\frac{4}{11} = .36$\n    -   SD = 0.1388659\n-   With prior hyperparameters $a=100$ and $b=500$\n    -   $\\hat{p}_1 = \\frac{100+3}{(100+3) + (500+2)} = \\frac{103}{605} = .17$\n    -   SD =\n        0.0152679\n-   The standard deviation (SD) of the posterior distribution is\n    analogous to the standard error in frequentist statistics\n\n## Bayesian Updating\n\nWe can use the posterior distribution as a prior!\n\nLet's roll a die to find out how...\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](01_Introduction_to_Bayesian_files/figure-html/updating-1.png){width=672}\n:::\n:::\n\n\n## Wrapping Up\n\nToday was a very quick introduction to Bayesian concepts:\n\n-   <u>prior distribution</u>\n    -   <u>hyperparameters</u>\n    -   <u>informative/uninformative</u>\n    -   <u>conjugate prior</u>\n-   <u>data likelihood</u>\n-   <u>posterior distribution</u>\n-   Next we will discuss psychometric models and how they fit into\n    Bayesian methods\n",
    "supporting": [
      "01_Introduction_to_Bayesian_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}