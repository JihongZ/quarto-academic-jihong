{
  "hash": "17dc3bfa762dafca9f9ef06f7214fa9b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Lecture 02\"\nsubtitle: \"Introduction to Bayesian Concepts\"\nauthor: \"Jihong Zhang\"\ninstitute: \"Educational Statistics and Research Methods\"\ntitle-slide-attributes:\n  data-background-image: ../Images/title_image.png\n  data-background-size: contain\n  data-background-opacity: \"0.9\"\nformat: \n  revealjs:\n    logo: ../Images/UA_Logo_Horizontal.png\n    incremental: true  # choose \"false \"if want to show all together\n    theme: [serif, pp.scss]\n    footer:  <https://jihongzhang.org/posts/2024-01-12-syllabus-adv-multivariate-esrm-6553>\n    transition: slide\n    background-transition: fade\n    slide-number: true\n    chalkboard: true\n    multiplex: true\n    number-sections: false\n---\n\n\n## Today's Lecture Objectives\n\n1.  Bayes' Theorem\n\n2.  Likelihood function, Posterior distribution\n\n3.  How to report posterior distribution of parameters\n\n4.  Bayesian update\n\nbut, before we begin...\n\n## Quiz: What is Bayesian?\n\n1.  What are key components of Bayesian models?\n\n    1.  Likelihood function - **data**\n    2.  Prior distribution - belief / previous evidences of **parameters**\n    3.  Posterior distribution - information of parameters give our data\n    4.  Posterior predictive distribution - future / predicted data\n\n2.  What are the differences between Bayesian with Frequentist analysis?\n\n    1.  prior distribution: Bayesian\n    2.  hypothesis of fixed parameters: frequentist\n    3.  estimation process: MCMC vs. MLE\n    4.  posterior distribution vs. point estimates of parameters\n    5.  credibles interval (plausibility of the parameters having those values) vs. confidence interval (the proportion of infinite samples having the fixed parameters)\n\n## Bayes' Theorem: How Bayesian Statistics Work\n\nBayesian methods rely on Bayes' Theorem\n\n$$\nP(\\theta | Data) = \\frac{P(Data|\\theta)P(\\theta)}{P(Data)} \\propto P(Data|\\theta) P(\\theta)\n$$\n\nWhere:\n\n1.  P: probability distribution function (PDF)\n2.  $P(\\theta|Data)$ : the [posterior distribution]{.underline} of parameter $\\theta$, given the observed data\n3.  $P(Data|\\theta)$: the likelihood function (conditional distributin) of the observed data, given the parameters\n4.  $P(\\theta)$: the [prior distribution]{.underline} of parameter $\\theta$\n5.  $P(Data)$: the marginal distribution of the observed data\n\n------------------------------------------------------------------------\n\n**A Live Bayesian Example**\n\n-   Suppose we want to assess the probability of rolling a \"1\" on a six-sided die:\n\n    $$\n    \\theta \\equiv p_{1} = P(D = 1)\n    $$\n\n-   Suppose we collect a sample of data (N = 5):\\\n    $$Data \\equiv X = \\{0, 1, 0, 1, 1\\}$$\n\n-   The [prior distribution]{.underline} of parameters is denoted as $P(p_1)$ ;\n\n-   The [likelihood function]{.underline}is denoted as $P(X | p_1)$;\n\n-   The [posterior distribution]{.underline} is denoted as $P(p_1|X)$\n\n-   Then we have:\n\n    $$\n    P(p_1|X) = \\frac{P(X|p_1)P(p_1)}{P(X)} \\propto P(X|p_1) P(p_1)\n    $$\n\n------------------------------------------------------------------------\n\n-   The Likelihood function $P(X|p_1)$ follows Bernoulli distribution of N =5 samples:\\\n    $$P(X|p_1) = \\prod_{i =1}^{N=5} p_1^{X_i}(1-p_1)^{X_i} \n    \\\\= (1-p_i) \\cdot p_i \\cdot (1-p_i) \\cdot p_i \\cdot p_i$$\n\n-   Question here: WHY USE BERNOULLI DISTRIBUTION?\n\n-   My answer: Bernoulli dist. has nice statistical probability. \"Nice\" means making sense in normal lifeâ€“ a common belief. For example, the $p_1$ value that maximizes the Bernoulli-based likelihood function is $Mean(X)$, and the $p_1$ values that minimizes the Bernouli-based likelihood function is 0 or 1\n\n------------------------------------------------------------------------\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Lecture02_files/figure-revealjs/unnamed-chunk-1-1.png){width=960}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.6\n```\n\n\n:::\n:::\n\n\n### Bayesian Thinking Process\n\n#### A toy example:\n\nI originally thought the ratio of male Asian faculty to female is about 1:2 (**Prior**). But the data we sampled from 2000 doctoral students suggested gender ratio is 1:1 (**Data; Likelihood that the true ratio we don't know**). Based on Bayes's rule, estimate for the ratio is probably 1:1.5 (**Posterior**) after combining these two statements.\n\n## Bayesian Analysis: Why It Is Used?\n\nThere are at least four main reasons why people use Bayesian Analysis:\n\n1.  Missing data\n    -   Multiple imputation\n    -   More complicated model for certain types of missing data\n2.  Lack of software capable of handing large sized analyses\n    -   Have a zero-inflated Poisson model with 1000 observations and 1000 parameters? No problem in Bayesian!\n\n## Bayesian Analysis: Why It Is Used? (Cont.)\n\n3.  New complex models not available in frequentist framework\n    -   Have a new model? (A model that estimates the probability students choose the right answers then choose the wrong answers in a multiple choice test?)\n4.  Enjoy the Bayesian thinking process\n    -   It is a way of thinking that everything is random and everything can be expressed as probability. It is a way of thinking that we can update our belief as we collect more data. It is a way of thinking that we can use our prior knowledge to help us understand the data.\n\n## Bayesian Analysis: Why It Is Used? (Cont.)\n\n![](/posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Images/DAG_BN.png)\n\n## Bayesian Analysis: Issues\n\n1.  Subjective vs. Objective\n    -   Prior distribution is subjective. It is based on your prior knowledge.\n    -   However, 1) Scientific judgement is always subjective 2) you can use objective prior distribution to avoid this issue.\n2.  Computationally Intensive\n    -   It is not a problem anymore. We have computers.\n    -   But we still need weeks or months to get results for some complated model and big data\n3.  Difficult to understand\n\n## Bayesian Analysis is popular\n\n![Funding available only for NIH, CDC, FDA, AHRQ, and ACF 2020 Spring. Source: https://report.nih.gov/](../Images/histogram_fundingamount.png){width=\"100%\"}\n\n## What topics Bayesian Analysis can cover?\n\n![Funding available only for NIH, CDC, FDA, AHRQ, and ACF 2020 Spring. Source: https://report.nih.gov/](../Images/circle_topics.png){width=\"100%\"}\n\n## Next Class\n\nWe will talk about how Bayesian methods works in a little bit more technical way.\n\n## Suggestions\n\nYour opinions are very important to me. Feel free to let me know if you have any suggestions on the course.\n",
    "supporting": [
      "Lecture02_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}