{
  "hash": "c2b52e4969c00cef5c78af948d097398",
  "result": {
    "markdown": "---\ntitle: Lasso Regression Example using glmnet package in R\nauthor: Jihong Zhang\ndate: '2019-02-19'\ncategories:\n  - R\n  - manual\ntags:\n  - Lasso\n  - glmnet\noutput: html_document\n---\n\n\n\n\nMore details please refer to the link below:\n(https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html#lin)\n\n\nThis post shows how to use `glmnet` package to fit lasso regression and how to visualize the output.\nThe description of data is shown in [here](http://myweb.uiowa.edu/pbreheny/data/whoari.html).\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndt <- readRDS(url(\"https://s3.amazonaws.com/pbreheny-data-sets/whoari.rds\"))\nattach(dt)\nfit <- glmnet(X, y)\n```\n:::\n\n\n## Visualize the coefficients\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(fit)\n```\n\n::: {.cell-output-display}\n![](index.en_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n### Label the path\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(fit, label = TRUE)\n```\n\n::: {.cell-output-display}\n![](index.en_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nThe summary table below shows from left to right the number of nonzero coefficients (DF), the percent (of null) deviance explained (%dev) and the value of $\\lambda$ (`Lambda`). \n\nWe can get the actual coefficients at a specific $\\lambda$ whin the range of sequence:\n\n::: {.cell}\n\n```{.r .cell-code}\ncoeffs <- coef(fit, s = 0.1) \ncoeffs.dt <- data.frame(name = coeffs@Dimnames[[1]][coeffs@i + 1], coefficient = coeffs@x) \n\n# reorder the variables in term of coefficients\ncoeffs.dt[order(coeffs.dt$coefficient, decreasing = T),]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          name   coefficient\n1  (Intercept)  3.010501e+00\n7          hfa  4.634502e-01\n12         aro  2.139111e-01\n9          inc  1.298769e-01\n6       convul  9.284738e-02\n16         whz  9.018865e-02\n14         afe  8.749033e-02\n10         lcw  7.538421e-02\n15        absu  5.758727e-02\n13         qcr  2.952464e-02\n11         str  2.298848e-02\n3         temp  9.842032e-03\n5           rr  7.309241e-03\n8          hfe  4.870858e-03\n2         wght -3.138053e-05\n4          age -8.724823e-03\n```\n:::\n:::\n\n\nAlso, it can allow people to make predictions at specific $\\lambda$ with new input data:\n\n::: {.cell}\n\n```{.r .cell-code}\nnx = matrix(rnorm(nrow(dt$X)*ncol(dt$X)), nrow = nrow(dt$X), ncol = ncol(dt$X))\npred <- predict(fit, newx = nx, s = c(0.1, 0.05)) \nhead(pred, 20)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            s1         s2\n [1,] 3.097967  1.1490660\n [2,] 3.121528  1.8672064\n [3,] 3.481338  2.0824025\n [4,] 3.513677  1.9860360\n [5,] 3.214442  1.7452629\n [6,] 2.490538  0.6267712\n [7,] 2.469592  0.9068895\n [8,] 2.720040  1.2436652\n [9,] 3.849610  1.9736908\n[10,] 3.033293  1.1007648\n[11,] 2.842930  1.0094204\n[12,] 3.027152  1.5992459\n[13,] 3.094639  0.7926862\n[14,] 2.082463 -1.1104287\n[15,] 3.612934  1.1337165\n[16,] 3.122380  1.8367326\n[17,] 2.133454  1.5344259\n[18,] 3.207939  0.4061563\n[19,] 3.138387  1.1810681\n[20,] 2.241477 -0.6124045\n```\n:::\n:::\n\n\n`cv.glmnet` is the function to do cross-validation here.\n\n::: {.cell}\n\n```{.r .cell-code}\nX <- dt$X\ny <- dt$y\ncv.fit <- cv.glmnet(X, y)\n```\n:::\n\n\nPlotting the object gives the selected $\\lambda$ and corresponding Mean-Square Error.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(cv.fit)\n```\n\n::: {.cell-output-display}\n![](index.en_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\nWe can view the selected $\\lambda$'s and the corresponding coefficients, For example,\n\n::: {.cell}\n\n```{.r .cell-code}\ncv.fit$lambda.min\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.02751064\n```\n:::\n\n```{.r .cell-code}\ncv.fit$lambda.1se\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.08401352\n```\n:::\n:::\n\n\n`lambda.min` returns the value of $\\lambda$ that gives minimum mean cross-validated error. The other $\\lambda$ saved is `lambda.lse`, which gives the most regularized model such that error is within one standard error of the minimum. To use that, we only need to replace `lambda.min` with `lambda.lse` above.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create a function to transform coefficient of glmnet and cvglmnet to data.frame\ncoeff2dt <- function(fitobject, s) {\n  coeffs <- coef(fitobject, s) \n  coeffs.dt <- data.frame(name = coeffs@Dimnames[[1]][coeffs@i + 1], coefficient = coeffs@x) \n\n  # reorder the variables in term of coefficients\n  return(coeffs.dt[order(coeffs.dt$coefficient, decreasing = T),])\n}\n\ncoeff2dt(fitobject = cv.fit, s = \"lambda.min\") %>% head(20)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          name coefficient\n19         str 0.624028697\n30         whz 0.552566043\n10         hfa 0.520301406\n9       convul 0.367161468\n24         aro 0.305231265\n32      puskin 0.246791663\n12         fde 0.204677903\n26         afe 0.162886449\n16         inc 0.142234777\n34         oab 0.132001767\n1  (Intercept) 0.126502256\n11         hfe 0.109667313\n4         temp 0.084079719\n18         lcw 0.075124732\n31        smi2 0.051082663\n25         qcr 0.048926715\n17         sr1 0.019747399\n28         crs 0.014597356\n20         gru 0.008735240\n7           rr 0.007203584\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncoeffs.table <- coeff2dt(fitobject = cv.fit, s = \"lambda.min\")\nggplot(data = coeffs.table) +\n  geom_col(aes(x = name, y = coefficient, fill = {coefficient > 0})) +\n  xlab(label = \"\") +\n  ggtitle(expression(paste(\"Lasso Coefficients with \", lambda, \" = 0.0275\"))) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        legend.position = \"none\") \n```\n\n::: {.cell-output-display}\n![](index.en_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n## Elastic net\nAs an example, we can set $\\alpha=0.2$\n\n::: {.cell}\n\n```{.r .cell-code}\nfit2 <- glmnet(X, y, alpha = 0.2, weights = c(rep(1, 716), rep(2, 100)), nlambda = 20)\n\nprint(fit2, digits = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:  glmnet(x = X, y = y, weights = c(rep(1, 716), rep(2, 100)), alpha = 0.2,      nlambda = 20) \n\n   Df  %Dev Lambda\n1   0  0.00 3.2500\n2   7 14.02 2.0000\n3  12 26.98 1.2300\n4  13 33.81 0.7590\n5  18 38.03 0.4670\n6  23 41.35 0.2880\n7  29 43.24 0.1770\n8  39 45.05 0.1090\n9  47 46.17 0.0672\n10 52 46.82 0.0414\n11 57 47.15 0.0255\n12 58 47.29 0.0157\n13 60 47.35 0.0097\n14 60 47.38 0.0060\n15 64 47.39 0.0037\n16 65 47.39 0.0023\n17 66 47.40 0.0014\n18 66 47.40 0.0009\n19 66 47.40 0.0005\n```\n:::\n:::\n\n\nAccording to the default internal settings, the computations stop if either the fractional change in deviance down the path is less than $10^{-5}$ or the fraction of explained deviance reaches 0.999.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(fit2, xvar = \"lambda\", label = TRUE)\n```\n\n::: {.cell-output-display}\n![](index.en_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# plot against %deviance\nplot(fit2, xvar = \"dev\", label = TRUE)\n```\n\n::: {.cell-output-display}\n![](index.en_files/figure-html/unnamed-chunk-11-2.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(fit2, newx = X[1:5, ], type = \"response\", s = 0.03)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           s1\n[1,] 3.170055\n[2,] 4.869413\n[3,] 4.255960\n[4,] 5.018313\n[5,] 2.945014\n```\n:::\n:::\n",
    "supporting": [
      "index.en_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}