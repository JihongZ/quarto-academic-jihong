---
title: "Lecture 05"
subtitle: "Bayesian Model fit and comparison"
author: "Jihong Zhang"
institute: "Educational Statistics and Research Methods"
title-slide-attributes:
  data-background-image: ../Images/title_image.png
  data-background-size: contain
  data-background-opacity: "0.9"
execute: 
  echo: true
format: 
  revealjs:
    logo: ../Images/UA_Logo_Horizontal.png
    incremental: true  # choose "false "if want to show all together
    theme: [serif, ../pp.scss]
    footer:  <https://jihongzhang.org/posts/2024-01-12-syllabus-adv-multivariate-esrm-6553>
    transition: slide
    background-transition: fade
    slide-number: true
    chalkboard: true
    number-sections: false
    code-line-numbers: true
    code-link: true
    code-annotations: hover
    code-copy: true
    highlight-style: arrow
    code-block-border-left: true
    code-block-background: "#b22222"
bibliography: references.bib
---

## Today's Lecture Objectives

1.  Bayesian methods for determining how well a model fits the data (absolute fit)

2.  Bayesian methods for determining which model fits better (relative model fit)

3.  In previous class...

    1.  We estimated the empty model and the full model (you can try other constrained model between the empty and full model)

    2.  We also make the Stan code more efficient by vectorizing the parameters and the data

    3.  The next question is how to determine the model is "good" enough

## Absolute Model Fit: PPMC

**Posterior predictive model checking** (PPMC) is a Bayesian method for determining if a model fits the data.

-   Absolute model fit: "Does my model fit my data well?"

    -   Recall that "model is a simplified version of the true data-generation process"

    -   Thus, the model should be able to reproduce the "data" that is similar to observed data

    -   In machine learning, this is also called "validation", typically used as a separate "validation data sets"

-   Overall idea: if a model fits the data well, then simulated data based on the model will resemble the observed data

------------------------------------------------------------------------

### Ingredients in PPMC:

::: nonincremental
-   Original data

    -   Typically, characterized by some set of statistics (i.e., sample mean, standard deviation, covariance) applied to data

-   Simulated data

    ::: nonincremental
    -   Generated from partial/all posterior draws in the Markov Chain

    -   Summarized by the same set of statistics
    :::
:::

![](/posts/2024-01-12-syllabus-adv-multivariate-esrm-6553/Images/Lecture5_PPMC_DataSets.png){fig-align="center"}

------------------------------------------------------------------------

## PPMC Example: Linear Models

The linear model from our example was:

$$
\text{WeightLB}_p = \beta_0 + \beta_1 \text{HeightIN}_p + \beta_2 \text{Group2}_p + \beta_3\text{Group3}_p \\ 
+\beta_4 \text{HeightIN}_p\text{Group2}_p \\
+\beta_5 \text{HeightIN}_p\text{Group3}_p \\
+ e_p
$$

with:

::: nonincremental
-   $\text{Group2}_p$ the binary indicator of person $p$ being in group 2
-   $\text{Group}3_p$ the binary indicator of person $p$ being in group 3
-   $e_p \sim N(0, \sigma_e)$
:::

```{r}
#| echo: false
load(here::here("posts", "2024-01-12-syllabus-adv-multivariate-esrm-6553", "Lecture05", "Code","Lecture05.RData"))
```

```{r}
fit_full_new$summary(variables = c('beta', 'sigma'))
```

------------------------------------------------------------------------

### PPMC Process

The PPMC Process can be summarized as follows:

1.  Select parameters from a single (sampling) iteration of the Markov Chain
2.  Using the selected parameters and the model to simulate a data set with the sample size (with same number of observations/variables)
3.  From the simulated data set, calculate selected summary statistics (e.g., mean, sd)
4.  Repeat steps 1-3 for a fixed number of iterations (perhaps across the whole chain)
5.  When done, compare position of observed summary statistics to that of the distribution of summary statistics from simulated data sets (predictive distribution)

------------------------------------------------------------------------

### Step 1: Assemble the posterior draws

For our linear model, let's denote our observed dependent variable as $Y$

-   Note that independent variables are not modeled (not explained by statistical formula), so we cannot examine them.

First, let's assemble the posterior draws ($1000 \times 4 \times 7$):

```{r}
posteriorSample = fit_full_new$draws(variables = c('beta','sigma'), format = 'draws_matrix')
posteriorSample
```

------------------------------------------------------------------------

### Step 2: One draw from posterior

Next, let's draw one set of posterior values of parameters at random with replacement:

```{r}
set.seed(1234)
sampleIteration = sample(x = 1:nrow(posteriorSample), size = 1, replace = TRUE)
sampleIteration
posteriorSample[sampleIteration,]
```

Those draws of $\boldsymbol{\beta}$ and $\sigma$ can be used to generate a predicted $\hat{y}$:

$$
\hat{y}_i \sim N(\mathbf{X}\boldsymbol{\beta}_i, \sigma_i)
$$

-   $i$ represents the $i$ th draw

------------------------------------------------------------------------

### Step 3: One simulated data

We then generate data based on this sampled iteration and our model distributional assumption:

```{r}
# Sample Size
N = nrow(dat)
# beta
betaVector = matrix(posteriorSample[sampleIteration, 1:6], ncol = 1)
betaVector
# sigma
sigma = posteriorSample[sampleIteration, 7]
# X
FullModelFormula = as.formula("WeightLB ~ HeightIN60 + DietGroup + HeightIN60*DietGroup")
X = model.matrix(FullModelFormula, data = dat)

simY = rnorm(n = N, mean = X %*% betaVector, sd = sigma)
head(simY)
```

------------------------------------------------------------------------

### Step 4: Summary Statistics of Simulated Data

Note that we do not want to directly compare simulated data to the observed data.

Instead, we extract some characteristics of simulated/observed data for comparison using summary statistics.

There are some advantages:

1.  We may have research interests in only some characteristics of data (whether our model predict the mean of dependent variables)
2.  We can QC detailed aspects of the fitting process of model

In this case, for example, we may be interested in whether the model captures the "location" or "scale" of WeightLB

```{r}
mean(dat$WeightLB)
(simMean = mean(simY))
sd(dat$WeightLB)
(simSD = sd(simY))
```

------------------------------------------------------------------------

### Step 5: Looping across all posterior samples

We can repeat step 2-4 for a set number of samples

Optionally, we can choose to use up all iterations we have for Markov Chains ($I = 4000$) in practice

```{r}
#| output-location: slide
I = nrow(posteriorSample)
## create empty simSD and simMean as placeholders
simSD = simMean = rep(NA, I)
for (i in 1:I) {
  # beta
  betaVector = matrix(posteriorSample[i, 1:6], ncol = 1)
  # sigma
  sigma = posteriorSample[i, 7]
  # X
  simY = rnorm(n = N, mean = X %*% betaVector, sd = sigma)
  simMean[i] = mean(simY)
  simSD[i] = sd(simY)
}
par(mfrow = 1:2)
hist(simMean)
hist(simSD)
```

------------------------------------------------------------------------

### Compare to the observed mean

::: columns
::: {.column width="50%"}
We can now compare our observed mean and standard deviation with that of the sample values.

-   Blue line: the average value of predicted WeightLB

-   Red line: the observed mean value of WeightLB

-   The PDF of predictive values of summary statistics of WeightLB is called `posterior predictive distribution`
:::

::: {.column width="50%"}
PPMC can be checked using visual inspection:

```{r}
library(ggplot2)
ppp <- data.frame(
  simMean = simMean, 
  simSD = simSD
)
ggplot(ppp) +
  geom_histogram(aes(x = simMean), fill = "grey", col = "black") +
  geom_vline(xintercept = mean(dat$WeightLB), col = "red", size = 1.4) + # red line: location of mean of predicted WeightLB by model
  geom_vline(xintercept = mean(simMean), col = "blue", size = 1.4, alpha = 0.5) # blue line: location of mean of WeightLB
```
:::
:::

------------------------------------------------------------------------

### Compare to the observed SD

::: columns
::: {.column width="50%"}
Similarly, let's compare SD to the posterior predictive distribution of SD of WeightLB

-   the observed SD is located as the center of posterior predictive distribution (PPD)

-   the average mean of PPD is slightly higher than the observed SD
:::

::: {.column width="50%"}
```{r}
ggplot(ppp) +
  geom_histogram(aes(x = simSD), fill = "grey", col = "black") +
  geom_vline(xintercept = sd(dat$WeightLB), col = "red", size = 1.4) + # red line: location of mean of predicted WeightLB by model
  geom_vline(xintercept = mean(simSD), col = "blue", size = 1.4, alpha = 0.5) # blue line: location of mean of WeightLB
```
:::
:::

------------------------------------------------------------------------

## PPMC Characteristics

PPMC methods are very useful

-   They provide a visual way to determine if the model fits the observed data

-   They are the main method of assessing absolute fit in Bayesian models

-   Absolute fit assesses if a model fits the data instead of comparing to another model

But, there are some drawbacks to PPMC methods

-   Almost any statistic can be used

    -   Some are better than others (mean and SD of outcomes are nice choices for linear regression)

-   No standard determining how much misfit is too much

-   May be overwhelming to compute depending on your model

------------------------------------------------------------------------

## Posterior Predictive P-values

::: columns
::: {.column width="50%"}
We can summarize the PPMC using a type of "p-value"

> Personally, I don't like the name "p-value", sounds like we are trying to justify our results using significance testing

Different from the frequentist "p-value" (if the null hypothesis is true, the probability of the observed data existing)

-   The PPP-value: the proportion of times the statistic from the simulated data exceeds that of the observed data

-   Useful to determine how far off a statistic is from its posterior predictive distribution
:::

::: {.column width="50%"}
If these p-values were:

1.  near 0 or 1, indicating your model is far off your data
2.  near .5, indicating your model fits your data in terms of the statistics you examined

The PPP-value for mean:

```{r}
mean(simMean > mean(dat$WeightLB))
```

The PPP-value for SD:

```{r}
mean(simSD > sd(dat$WeightLB))
```
:::
:::

------------------------------------------------------------------------

## Compute PPP-values within Stan

We can use the `generated quantities` block of Stan to compute PPP-values for us:

```{stan output.var='display', eval = FALSE, tidy = FALSE}
generated quantities{
  // simulated data
  array[N] real weightLB_rep = normal_rng(X*beta, sigma);
  // posterior predictive distribution for mean and SD
  real mean_weightLB = mean(weightLB);
  real sd_weightLB = sd(weightLB);
  real mean_weightLB_rep = mean(to_vector(weightLB_rep));
  real<lower=0> sd_weightLB_rep = sd(to_vector(weightLB_rep));
  // ppp-values for mean and sd
  int<lower=0, upper=1> ppp_mean = (mean_weightLB_rep > mean_weightLB);
  int<lower=0, upper=1> ppp_sd = (sd_weightLB_rep > sd_weightLB);
}
```

It will give us:

```{r}
fit_full_ppp$summary(variables = c('mean_weightLB_rep', 'sd_weightLB_rep', 'ppp_mean', 'ppp_sd'))
```

------------------------------------------------------------------------

## Advantages or disadvatages of Computing PPP within Stan

::: columns
::: {.column width="60%"}
Pros:

1.  Built-in functions of Stan to generate simulated data for example `normal_rng()`, making PPP-values estimated much faster
2.  Nice visual inspection tools existed - `bayesplot`

Cons:

1.  Not allowed to debug each step in PPMC if something wrong
2.  Cannot adjust the statistics and need to re-run the whole MCMC sampling, which is time-consuming
:::

::: {.column width="40%"}
```{r}
#| fig-cap: 'Posterior predictive distribution for mean of weight by chains'
bayesplot::mcmc_dens_chains(fit_full_ppp$draws('mean_weightLB_rep'))
```
:::
:::

------------------------------------------------------------------------

## Relative Model Fit

Relative model fit: used to compare 2 or more competing models in terms of their mode fit. Sometime, it is also called model selection.

-   In non-Bayesian models, Information Criteria are often used to make comparisons

    -   AIC, BIC, DIC etc.

    -   Typically IC is a function of log-likelihood and penality

    -   The model with the lowest IC is the model that fits best

-   Bayesian model fit is similar

    -   Uses an index value

    -   The model with the lowest index is the model that fits best

-   Recent advances in Bayesian model fit use indices that are tied to make cross-validation predictions (inspired by machine learning):

    -   Fit model leaving one observation out (LOO)

    -   Calculate statistics related to prediction (for instance, log-likelihood of that observation conditional on model parameters)

    -   Do for all observations

-   New Bayesian indices try to mirror these leave-one-out predictions (but approximate these due to time constraints)

------------------------------------------------------------------------

## Deviance Information Indices

When late 1990s and early 2000s, the **Deviance Information Criterion** was popular for relative Bayesian model fit comparisons. It is proved not as good as LOO or WAIC. But let's have a look at:

$$
\text{DIC} = p_D + \overline{D(\theta)}
$$

where $p_D$ is the estimated number of parameters as follows:

$$p_D = \overline{D(\theta)} - D(\bar\theta)$$and where

------------------------------------------------------------------------

## Introduction: Approximation to Leave-one-out 

Big picture:

1.  This comparative fit indices include WAIC, LOO, Pareto Smoothed Important Sampling (PSIS) via Stan's `LOO` package
2.  WAIC/LOO can be used for model comparison with lowest value suggesting better model fit
3.  Different from AIC/BIC/DIC, LOO via PSIS attempts to "approximate" the process of leave-one-out cross-validation (LOO-CV) using a sampling based-approach
    -   Gives a finite-sample approximation
    -   Implemented in Stan
    -   Can quickly compare models
    -   Gives warnings when it may be less reliable to use

------------------------------------------------------------------------

## WAIC in Stan

------------------------------------------------------------------------

## LOO: Leave-one-out in Stan

Using `loo` package, Efficient approximate leave-one-out cross-validation (LOO-CV)

::: columns
::: {.column width="50%"}
Full model's LOO:

```{r}
#| echo: false
library(loo)
```

```{r}
#| warning: false
full_loo_res <- fit_full_ppp$loo('log_lik', save_psis = TRUE)
full_loo_res$estimates
```
:::

::: {.column width="50%"}
Empty model's LOO:

```{r}
empty_loo_res <- fit_empty_ppp$loo('log_lik', save_psis = TRUE)
empty_loo_res$estimates
```
:::
:::

Here:

-   `elpd_loo` is the expected log pointwise predictive density for LOO (recall that posterior predictive distribution has some uncertainty around the mean value...)

-   `p_loo` is the LOO calculation of number of model parameters (a penalty to the likelihood for more parameters)

-   `looic` is the LOO index used for model comparisons â€” lowest value suggests best fitting -2`elpd_loo`

------------------------------------------------------------------------

## LOO: Pareto smoothed importance sampling (PSIS)

-   Estimated a vector of Pareto shape parameters $k$ for individuals which represents the reliability of sampling:

    -   $k < .5$ (good) suggests the estimate converges quickly

    -   $.5 < k < .7$ (ok) suggests the estimate converges slowly

    -   $.7 < k < 1$ (bad) suggests bad performance

    -   $k > 1$ (very bad)

-   PSIS screens all cases that has had bad diagnostic values

```{r}
pareto_k_table(full_loo_res)
```

Criteria: Estimated tail shape parameter

This is new area, please see more references [@vehtari2016]

------------------------------------------------------------------------

## Wrapping up

## Next Class

1.  Bayesian Model fit
2.  Bayesian Model Comparison
