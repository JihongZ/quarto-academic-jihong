---
title: "How to set up High Performance Computing of University of Arkansas"
author: Jihong Zhang
date: 2024-01-14
---

## General Information

Arkansas High Performance Computing Center (AHPCC) is available for research and instructional use to faculty and students of any Arkansas university and their research collaborators. There is no charge for use of our computing resources.

To use the HPC, an AHPCC account must be requested through [Internal Account Request Form](http://hpc.uark.edu/hpc-support/user-account-requests/internal.php). Please see [here](https://dartproject.org/hpc/) for more information about AHPPC inventory.

## Connect to HPC

As long as you have an AHPCC account, you can connect to HPC through SSH. For Windows users, you can use [PuTTY](http://www.putty.org/) to connect to HPC. For Mac and Linux users, you can use the terminal to connect to HPC. The command is:

``` bash
ssh [loginname]@hpc-portal2.hpc.uark.edu
```

If your account was successfully setted up, passwords will be required. After you enter your password, you will be connected to HPC.

![Login Screenshot](login.png)

Note: Pinnacle is a new resource at the University of Arkansas in 2019. It consists of 100 Intel based nodes with 20 NVIDIA V100 GPU nodes enabling data science and machine learning and 8 big memory nodes with 768 Gb ram/each for projects requiring a large memory footprint.

### SSH login without password

1.  Generate a pair of authentication keys in your local machine and do not enter a passphrase using the following codes:

``` bash
ssh-keygen -t rsa   
```

Please note that make the passphrase empty:

```         
Generating public/private rsa key pair.
Enter file in which to save the key (/Users/[username]/.ssh/id_rsa): 
Enter passphrase (empty for no passphrase): 
Enter same passphrase again: 
Your identification has been saved in /Users/[username]/.ssh/id_rsa
Your public key has been saved in /Users/[username]/.ssh/id_rsa.pub
```

2.  In your local machine, type in following commands to copy local public key to the hpc server.

``` bash
scp ~/.ssh/id_rsa.pub [loginname]@hpc-portal2.hpc.uark.edu:/home/[loginname]/.ssh/authorized_keys
```

3.  Now you should be able to login the hpc login node without password:

``` bash
ssh [loginname]@hpc-portal2.hpc.uark.edu
```

## Moving data

For moving data files from local machine to HPC, type in following codes on your local machine:

``` bash
scp program.c loginname@hpc-portal2.hpc.uark.edu:/home/loginname/
```

To copy an entire directory tree `src` using SCP, use `-r` for recursive:

``` bash
scp -r src loginname@hpc-portal2.hpc.uark.edu:/home/loginname/
```

Similarly, for moving data files from HPC to local machine, type in following codes on your local machine:

``` bash
scp -r loginname@hpc-portal2.hpc.uark.edu:/home/loginname/src ./
```

## Jobs submission

Please refer to this [link](https://hpcwiki.uark.edu/doku.php?id=equipment) for detailed information about HPC equipment.

``` bash
sbatch -q q06h32c -l walltime=1:00 -l nodes=1:ppn=32 example.sh
```

`sbatch` command aims to submit a job with the job file `example.sh`. The command above submitted the job to the q06h32c queue with a wall-time of 1 minute requesting all 32 cores on 1 node.

Here's a simple example of a job script:

``` bash
#!/bin/bash
#SBATCH --job-name=mpi
#SBATCH --output=zzz.slurm
#SBATCH --partition comp06
#SBATCH --nodes=2
#SBATCH --tasks-per-node=32
#SBATCH --time=6:00:00
module load gcc/11.2.1 mkl/19.0.5 R/4.2.2

Rscript HelloWorld/example.R
```

Where `gcc` and `mkl` are required for R package installation (Note: To the date, gcc/11.2.1 is the latest version which can compile the `cmdstan` successfully.). Please see [here](https://hpcwiki.uark.edu/doku.php?id=r) for more details. `Rscript` is the command to execute R file. `HelloWorld/example.R` is the path of your R script. For `#SBATCH`, those are options for SLURM scheduler. Please see following summary or view [it](https://slurm.schedmd.com/pdfs/summary.pdf) online:

```{r, echo=FALSE}
xfun::embed_file("summary.pdf")
```

To use R interactively, please start an interactive job using following command:

``` bash
srun -N1 -n1 -c1 -p cloud72 -q cloud -t 2:00:00 --pty /bin/bash
```

This command will redirect to cloud72 queue, which includes virtual machines and containers, usually single processor, 72 hour limit, 3 nodes.

| slurm commands      | compatibility commands   | Meaning                            |
|---------------------|--------------------------|------------------------------------|
| sbatch              | qsub                     | submit \<job file\>                |
| srun                | qsub -I                  | submit interactive job             |
| squeue              | qstat                    | list all queued jobs               |
| squeue -u -rfeynman | qstat -u rfeynman        | list queued jobs for user rfeynman |
| scancel             | qdel                     | cancel \<job#\>                    |
| sinfo               | shownodes -l -n;qstat -q | node status;list of queues         |

For slurm, please use the commands in the 1st column. Then you should be able to start R in an interactive job and install required packages. If you load `R/4.2.2` module, those packages installed via an interactive job will be stored at `$HOME$/R/x86_64-pc-linux-gnu/4.2/`. See [here](https://hpcwiki.uark.edu/doku.php?id=pinnacle_usage) for more details about interactive job.

The workflow of job submission is as follows:

``` bash
c1331:jzhang:~/HelloWorld$ cat exampleJob.sh
#!/bin/bash
#SBATCH --nodes=1
#SBATCH --tasks-per-node=2
#SBATCH --job-name=Exp0
module purge
module load os/el7 gcc/9.3.1 mkl/19.0.5 R/4.2.2
cd $SLURM_SUBMIT_DIR
Rscript example.R
c1331:jzhang:~/HelloWorld$ rm slurm-334001.out
c1331:jzhang:~/HelloWorld$ sbatch exampleJob.sh
Submitted batch job 334002
c1331:jzhang:~/HelloWorld$ ls
exampleJob.sh  example.R  gene_up.txt  MS_entrez_id_alldata.txt  slurm-334002.out
c1331:jzhang:~/HelloWorld$
```

## Checking job

``` bash
squeue -u loginname
```

Below shows the information of all running/down task for the queue `comp06` (there are 9 Jobs but only 7 are running). Note that comp06 and comp72 queues share the same nodes.

```bash
✘ jzhang@pinnacle-l8  ~/Projects/DCM  squeue -p comp06
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
            362406    comp06    measr   jzhang PD       0:00      1 (Resources)
            362361    comp06 03a_iqtr amatthew PD       0:00      1 (Priority)
            362385    comp06 sys/dash ashmitau  R      36:05      1 c1402
            362334    comp06   fixed2    igorf  R    1:53:29      1 c1402
            362333    comp06   fixed1    igorf  R    1:53:32      1 c1402
            362332    comp06   fixed0    igorf  R    1:53:35      1 c1402
            362309    comp06 sys/dash    igorf  R    2:43:12      1 c1402
            362360    comp06 03a_iqtr amatthew  R      59:32      1 c1403
            362311    comp06  mystery cdgoolsb  R    2:37:50      1 c1410
            362310    comp06  mystery cdgoolsb  R    2:38:20      1 c1410
            362308    comp06  mystery cdgoolsb  R    2:43:21      1 c1410
```

When you want to get the worst case scenario estimate of when your waiting jobs will start, you can always run following command,

```bash
squeue -u [loginname] --start
```

### Troubleshooting

1.  Revise `.bashrc` so that the ssh cannot login?

    You potential can try `ctrl+c` to avoid the ssh to execute bashrc. Try multiple time if not succeed. See [here](https://serverfault.com/questions/206544/i-screwed-up-exit-in-bashrc) for reference.
