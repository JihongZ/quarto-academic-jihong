---
title: "How to set up High Performance Computing of U of Ark"
author: Jihong Zhang
date: 2024-01-14
---

## General Information

Arkansas High Performance Computing Center (AHPCC) is available for research and instructional use to faculty and students of any Arkansas university and their research collaborators. There is no charge for use of our computing resources.

To use the HPC, an AHPCC account must be requested through [Internal Account Request Form](http://hpc.uark.edu/hpc-support/user-account-requests/internal.php). Please see [here](https://dartproject.org/hpc/) for more information about AHPPC inventory.

## Connect to HPC

As long as you have an AHPCC account, you can connect to HPC through SSH. For Windows users, you can use [PuTTY](http://www.putty.org/) to connect to HPC. For Mac and Linux users, you can use the terminal to connect to HPC. The command is:

``` bash
ssh [loginname]@hpc-portal2.hpc.uark.edu
```

If your account was successfully setted up, passwords will be required. After you enter your password, you will be connected to HPC.

![Login Screenshot](login.png)

Note: Pinnacle is a new resource at the University of Arkansas in 2019. It consists of 100 Intel based nodes with 20 NVIDIA V100 GPU nodes enabling data science and machine learning and 8 big memory nodes with 768 Gb ram/each for projects requiring a large memory footprint.

## Moving data

For moving data files from local machine to HPC, type in following codes on your local machine:

``` bash
scp program.c loginname@hpc-portal2.hpc.uark.edu:/home/loginname/
```

To copy an entire directory tree `src` using SCP, use `-r` for recursive:

``` bash
scp -r src loginname@hpc-portal2.hpc.uark.edu:/home/loginname/
```

Similarly, for moving data files from HPC to local machine, type in following codes on your local machine:

``` bash
scp -r loginname@hpc-portal2.hpc.uark.edu:/home/loginname/src ./
```

## Jobs submission

Please refer to this [link](https://hpcwiki.uark.edu/doku.php?id=equipment) for detailed information about HPC equipment.

``` bash
sbatch -q q06h32c -l walltime=1:00 -l nodes=1:ppn=32 example.sh
```

`sbatch` command aims to submit a job with the job file `example.sh`. The command above submitted the job to the q06h32c queue with a wall-time of 1 minute requesting all 32 cores on 1 node.

Here's a simple example of a job script:

``` bash
#!/bin/bash
#SBATCH --job-name=mpi
#SBATCH --output=zzz.slurm
#SBATCH --partition comp06
#SBATCH --nodes=2
#SBATCH --tasks-per-node=32
#SBATCH --time=6:00:00
module load gcc/11.2.1 mkl/19.0.5 R/4.2.2

Rscript HelloWorld/example.R
```

Where `gcc` and `mkl` are required for R package installation. Please see [here](https://hpcwiki.uark.edu/doku.php?id=r) for more details. `Rscript` is the command to execute R file. `HelloWorld/example.R` is the path of your R script. For `#SBATCH`, those are options for SLURM scheduler. Please see following summary or view [it](https://slurm.schedmd.com/pdfs/summary.pdf) online:

```{r, echo=FALSE}
xfun::embed_file("summary.pdf")
```

To use R interactively, please start an interactive job using following command:

``` bash
srun -N1 -n1 -c1 -p cloud72 -q cloud -t 2:00:00 --pty /bin/bash
```

This command will redirect to cloud72 queue, which includes virtual machines and containers, usually single processor, 72 hour limit, 3 nodes.

| slurm commands      | compatibility commands   | Meaning                            |
|-------------------|-----------------------|-------------------------------|
| sbatch              | qsub                     | submit \<job file\>                |
| srun                | qsub -I                  | submit interactive job             |
| squeue              | qstat                    | list all queued jobs               |
| squeue -u -rfeynman | qstat -u rfeynman        | list queued jobs for user rfeynman |
| scancel             | qdel                     | cancel \<job#\>                    |
| sinfo               | shownodes -l -n;qstat -q | node status;list of queues         |

For slurm, please use the commands in the 1st column. Then you should be able to start R in an interactive job and install required packages. If you load `R/4.2.2` module, those packages installed via an interactive job will be stored at `$HOME$/R/x86_64-pc-linux-gnu/4.2/`. See [here](https://hpcwiki.uark.edu/doku.php?id=pinnacle_usage) for more details about interactive job.

The workflow of job submission is as follows:

``` bash
c1331:jzhang:~/HelloWorld$ cat exampleJob.sh
#!/bin/bash
#SBATCH --nodes=1
#SBATCH --tasks-per-node=2
#SBATCH --job-name=Exp0
module purge
module load os/el7 gcc/9.3.1 mkl/19.0.5 R/4.2.2
cd $SLURM_SUBMIT_DIR
Rscript example.R
c1331:jzhang:~/HelloWorld$ rm slurm-334001.out
c1331:jzhang:~/HelloWorld$ sbatch exampleJob.sh
Submitted batch job 334002
c1331:jzhang:~/HelloWorld$ ls
exampleJob.sh  example.R  gene_up.txt  MS_entrez_id_alldata.txt  slurm-334002.out
c1331:jzhang:~/HelloWorld$
```

## Checking job

``` bash
squeue -u loginname
```
